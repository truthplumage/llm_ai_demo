# [이론4] 벡터 검색

## 기본 개념
- **임베딩(Embedding)**: 텍스트(문장/문서)를 고차원 실수 벡터로 변환한 표현. 의미가 비슷한 문장은 벡터 공간에서 가깝게 위치.
- **코사인 유사도**: 두 벡터의 각도 유사도. `cos(θ) = (A·B) / (||A||*||B||)`, 1에 가까울수록 의미적으로 유사.
- **ANN(Approximate Nearest Neighbor)**: 대규모 벡터에서 근사 최근접을 빠르게 찾는 기법(HNSW, IVF 등). 정확도-속도 트레이드오프.

## 벡터 DB (Chroma 예시)
- **컬렉션(Collection)**: 벡터와 메타데이터, 원문을 묶어 저장하는 논리적 단위.
- **엔트리 필드**: `id`, `embedding`, `document`(원문), `metadata`(카테고리, 가격 등)를 함께 저장.
- **쿼리**: 입력 텍스트를 동일한 임베딩 모델로 변환 후 코사인 유사도 기준 상위 k개 반환.

## 구축 절차
1. **데이터 준비**: 문서 정규화/클렌징 후 문장 단위로 분할.
2. **임베딩 생성**: 동일 모델로 모든 문서와 질의 텍스트를 임베딩. (예: OpenAI text-embedding-3-small/large)
3. **업서트(Upsert)**: `id`, `embedding`, `metadata`와 함께 컬렉션에 적재.
4. **검색(Query)**: 질의 텍스트 임베딩 → k-NN 검색 → 상위 k개 반환.
5. **RAG 연결**: 검색된 문서를 컨텍스트로 LLM에 전달해 답변/추천 생성.

## 품질 최적화 포인트
- **토큰 단위 분할**: 너무 긴 문서는 의미가 희석되므로 200~500자/문장 단위로 쪼개고, 중복을 줄이기 위해 중첩 슬라이딩 윈도우 적용.
- **임베딩 모델 일관성**: 인덱싱과 쿼리에 동일 모델 사용. 다국어 지원이 필요하면 멀티링구얼 임베딩 사용.
- **메타데이터 필터**: 카테고리/가격대/재고 여부 등 메타 필터로 검색 범위를 축소해 정확도↑, 비용↓.
- **스코어 해석**: 코사인 유사도는 0~1 범위. 임계값(threshold)으로 낮은 스코어 결과를 필터링.
- **재랭킹**: 상위 후보에 대해 Cross-encoder 재랭킹 또는 LLM 재랭킹으로 품질 향상.

## 성능/운영 고려
- **인덱스 선택**: 데이터가 많을수록 HNSW/IVF 등 ANN 인덱스를 사용하여 검색 지연을 줄임.
- **메모리 관리**: 벡터 크기(차원 수)와 데이터량에 따라 메모리 요구가 커지므로 샤딩/파티셔닝 고려.
- **배치/실시간 적재**: 초기 벌크 업서트 후 변경분은 배치/스트리밍 파이프라인으로 동기화.
- **관측성**: 검색 지연, 상위 스코어 분포, 실패율, 리콜/정확도 모니터링.

## Chroma 간단 호출 흐름 (개념)
1. 컬렉션 생성: `createCollection("products")`
2. 업서트: `upsert(ids, embeddings, metadatas, documents)`
3. 질의: `query(queryEmbeddings, nResults=3, where=메타필터)`
4. 반환된 `documents/metadatas`를 LLM 컨텍스트로 전달해 답변/추천 생성.
