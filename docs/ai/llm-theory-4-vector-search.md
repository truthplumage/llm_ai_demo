# [이론4] 벡터 검색 (쉽고 자세하게)

## 한눈에 보는 흐름 (그림)
```
텍스트 → 임베딩(숫자 벡터) → 벡터DB 저장
질문 → 임베딩 → 벡터DB에서 가까운 벡터 찾기 → 원문을 LLM 프롬프트에 붙여 답변
```
- 느낌: “비슷한 맛”을 찾듯 숫자 공간에서 가장 가까운 것을 집어 LLM에 보여준다.

## 핵심 개념을 쉬운 말로
- **임베딩**: 문장을 좌표로 바꾸는 것. 비슷한 문장끼리 좌표가 가깝다.
- **코사인 유사도**: 두 좌표가 같은 방향을 얼마나 보는지(0~1). 1이면 거의 같은 뜻.
- **ANN 검색**: 데이터가 많을 때 “거의 가까운 것”을 빨리 찾는 알고리즘(HNSW, IVF 등).

## 단계별로 따라가기
1) **문서 쪼개기**: 200~500자 정도로 나눠 의미가 섞이지 않게 만든다.
2) **임베딩 생성**: 같은 임베딩 모델로 모든 문서와 질문을 숫자 벡터로 만든다.
3) **저장(업서트)**: `id + embedding + metadata + 원문`을 벡터 DB(예: Chroma, PGVector)에 넣는다.
4) **검색**: 질문 벡터로 k-NN 검색 → 상위 k개 문서를 받는다.
5) **LLM 호출**: 검색된 원문(또는 요약)을 프롬프트에 붙여 답변(RAG).

## 미니 예시 (텍스트가 어떻게 골라지는지)
```
문서 A: "강아지 사료 추천"
문서 B: "고양이 모래 구매"
질문: "멍멍이 먹을 만한 사료?"

임베딩 후 거리 비교 → 질문↔A는 가깝고, 질문↔B는 멀다.
=> A가 상위로 검색되고 LLM에 전달.
```

## 프롬프트 붙이기 예시
```
시스템: 너는 자료 기반으로만 답하는 도우미야.
사용자: 질문: {user_question}
관련 자료:
- {doc1}
- {doc2}
```
- 자료가 길면 상위 3~5개만 사용하거나 요약해서 넣는다.
- “자료에서 찾은 내용만 사용”을 명시해 할루시네이션을 줄인다.

## 자주 막히는 포인트 체크리스트
- **임베딩 모델 불일치**: 인덱싱과 쿼리 모델이 다르면 품질 급락 → 동일 모델 사용.
- **차원 불일치**: DB 컬럼 `vector(1536)`와 모델 출력 차원이 다르면 에러.
- **너무 긴 청크**: 1000자 이상이면 의미 희석 → 200~500자 + 약간의 중첩.
- **필터 누락**: 재고/상태 같은 구조화 값은 메타필터로 거르고, 임베딩에는 이름·설명 등 의미 텍스트만.
- **스코어 컷**: 유사도 낮은 결과는 0.75/0.8 같은 임계값으로 제거.

## 검색 스코어 해석
- 코사인 유사도: 0~1, 1에 가까울수록 유사.
- 거리 연산(`<=>`)을 쓰면 값이 **작을수록** 가깝다(정렬 방향 주의).
- 로그나 응답에 스코어를 함께 노출하면 튜닝에 도움이 된다.

## 운영 빠른 점검 루틴
1) 확장: `SELECT extname FROM pg_extension WHERE extname='vector';`
2) 직접 질의: `SELECT id, embedding <=> '[...]' AS score FROM product ORDER BY score LIMIT 3;`
3) 샘플 질문으로 API를 호출해, 프롬프트에 어떤 컨텍스트가 들어갔는지 로그 확인.

## Chroma 호출 흐름 (개념)
1. 컬렉션 생성: `createCollection("products")`
2. 업서트: `upsert(ids, embeddings, metadatas, documents)`
3. 질의: `query(queryEmbeddings, nResults=3, where=메타필터)`
4. 반환된 `documents/metadatas`를 LLM 컨텍스트로 전달해 답변/추천 생성.
